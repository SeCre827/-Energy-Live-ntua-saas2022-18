# Stress Testing

For the purposes of this project, we were asked to perform stress testing on the deployed software. The performed testing was not intended to be exhaustive and cover all cases, but rather its goal was simply to familiarise us with stress testing tools, as well as to examine how well our application can handle large amounts of traffic. 

For the purposes of stress testing, the [Apache JMeter](https://jmeter.apache.org/) tool was used. Through it, we generated and executed a Test Plan, which consists of a large number of HTTPS requests to be sent to our application in a short amount of time. Specifically, the Test Plan runs over 200 iterations. During each iteration, JMeter simulates 100 independent users (threads), each performing five HTTPS requests, one to the Frontend application and one to each of the microservices that the Frontend application communicates with (user-management, {atl, agpt, pf}-data-management). This behaviour simulates a user accessing the Frontend application, then extending their licence and finally viewing data for each of the three available datasets. We configure the Test Plan so that all 100 users in each iteration have begun sending requests within one second of the iteration starting, and also so that each user sends the five requests successively, without any delay in between. In total, the Frontend application and each of the four microservices receive 20,000 requests each.

In this directory, we present both the Test Plan, as well as the results of one typical run of it. This run terminated in 4'28". Of course, for such a short amount of time, sending 20,000 requests to each of the five above microservices represents an unreasonably large amount of traffic that should not be expected under normal circumstances. Even so, by examining the test results, we can conclude that our application can handle that workload effectively. Specifically, we observe that the error rate is zero for all microservices except the user-management one, which has a very low error rate of 0.03%. In other words, the large workload does not cause our application to fail while handling the requests. In addition, we see that all microservices generally maintain a consistently low response time, exhibiting only a small number of low-duration spikes, which of course are to be expected given the size of the workload. Overall, we can claim that our application can handle amounts of traffic much beyond what is normally expected, without exhibiting errors and while maintaining large request throughput, which is of course a satisfactory result.
